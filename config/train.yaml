batch_size: 8
grad_accum: 4
lr: 3.0e-4
min_lr: 3.0e-5
weight_decay: 0.1
warmup_steps: 2000
max_steps: 500000
log_interval: 50
save_interval: 1000
max_tokens_per_step: 8192
enable_grpo: true
group_size: 4
enable_prm: true
enable_continuous_batching: true
enable_fp8: false
grad_noise_std: 0.0
curriculum_start_len: 256
curriculum_end_len: 4096
loss_ce_weight: 1.0
loss_mtp_weight: 0.1
loss_contrastive_weight: 0.05
loss_distill_weight: 0.1
router_reg_weight: 0.01
activation_sparsity_weight: 0.001
token_entropy_weight: 0.001
logit_clip: 30.0
grad_clip_norm: 1.0
loss_ema_momentum: 0.95
fp8_grad_scale_init: 4096.0
lr_embed_mult: 0.5
lr_head_mult: 1.0
lr_moe_mult: 1.2
enable_speculative: true
enable_multi_draft: true
enable_mcts: true
max_batch_size: 16
speculative_steps: 4
mcts_simulations: 32
mcts_depth: 8
beam_width: 4
kv_max_tokens: 8192
kv_evict_keep: 2048
prefill_chunk_size: 1024
decode_chunk_size: 1
